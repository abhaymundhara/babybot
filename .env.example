# BabyBot Configuration

# Assistant name (will be used as trigger word: @Baby)
ASSISTANT_NAME=Baby

# Ollama Configuration
# URL where Ollama is running (default: local)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (must be pulled with: ollama pull <model>)
# Options: llama2, mistral, codellama, neural-chat, llama2:13b, etc.
OLLAMA_MODEL=llama2

# LLM Provider
# Options: ollama, openrouter
LLM_PROVIDER=ollama

# OpenRouter Configuration (used when LLM_PROVIDER=openrouter)
# OPENROUTER_API_KEY=sk-or-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=openai/gpt-4o-mini
# Optional attribution headers
# OPENROUTER_SITE_URL=https://your-site.example
# OPENROUTER_APP_NAME=BabyBot

# Container Runtime
# Options: auto (detect), apple-container, docker, none (direct execution)
CONTAINER_RUNTIME=auto
CONTAINER_IMAGE=babybot-agent:latest
CONTAINER_TIMEOUT=1800000
CONTAINER_MAX_OUTPUT_SIZE=10485760
# Optional mount allowlist path (defaults to ~/.config/babybot/mount-allowlist.json)
# MOUNT_ALLOWLIST_PATH=/Users/your-user/.config/babybot/mount-allowlist.json

# Logging
# Options: trace, debug, info, warn, error, fatal
LOG_LEVEL=info

# Advanced Configuration
# Number of recent chat messages included for follow-up context
CONVERSATION_CONTEXT_WINDOW=12

# How long to wait before considering a process idle (ms)
IDLE_TIMEOUT=1800000

# Maximum number of concurrent message processors
MAX_CONCURRENT_CONTAINERS=5

# Agent Swarms Configuration
# Enable multi-agent collaboration (experimental)
ENABLE_AGENT_SWARMS=false

# Maximum number of agents in a swarm
MAX_SWARM_SIZE=10

# Ollama experimental features for agent teams
OLLAMA_EXPERIMENTAL_AGENT_TEAMS=0
OLLAMA_ADDITIONAL_DIRECTORIES_MEMORY=0
OLLAMA_DISABLE_AUTO_MEMORY=0

# Timezone for scheduled tasks (uses system timezone by default)
# TZ=America/New_York

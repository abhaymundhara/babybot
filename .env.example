# BabyBot Configuration

# Assistant name (will be used as trigger word: @Baby)
ASSISTANT_NAME=Baby

# Ollama Configuration
# URL where Ollama is running (default: local)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (must be pulled with: ollama pull <model>)
# Options: llama2, mistral, codellama, neural-chat, llama2:13b, etc.
OLLAMA_MODEL=llama2

# Logging
# Options: trace, debug, info, warn, error, fatal
LOG_LEVEL=info

# Advanced Configuration
# How long to wait before considering a process idle (ms)
IDLE_TIMEOUT=1800000

# Maximum number of concurrent message processors
MAX_CONCURRENT_CONTAINERS=5

# Timezone for scheduled tasks (uses system timezone by default)
# TZ=America/New_York
